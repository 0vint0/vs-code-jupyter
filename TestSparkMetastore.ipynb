{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"scrolled": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Kerberos auth done.\n"}], "source": "%%bash \nkinit hive/ranger-master-test-m.us-central1-a.c.avid-reference-267100.internal@US-CENTRAL1-A.C.AVID-REFERENCE-267100.INTERNAL -t /etc/security/keytab/hive.service.keytab -k\necho \"Kerberos auth done.\""}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+\n|databaseName|\n+------------+\n|     default|\n|default_test|\n+------------+\n\n"}], "source": "import findspark\n\nfindspark.init()\n\nimport pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"show databases\")\n\ndf.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df = spark.sql(\"show tables\")\n\ndf.show()"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading Hive table\u2026\nRegistering DataFrame as a table\u2026\n+----+---+\n|name|age|\n+----+---+\n| abc| 23|\n| cdd| 44|\n+----+---+\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n\n"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cdd</td>\n      <td>44</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "  name  age\n0  abc   23\n1  cdd   44"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "import pandas as pd\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import HiveContext\n\nsqlContext = SQLContext.getOrCreate(spark)\n\n#Create a Hive Context\n\nhive_context = HiveContext(spark)\n\nprint (\"Reading Hive table\u2026\")\nsparkdf = hive_context.sql(\"SELECT * FROM customer\")\n\nprint (\"Registering DataFrame as a table\u2026\")\n\nsparkdf.show() # Show first rows of dataframe\n\nsparkdf.printSchema()\n\nsparkdf.limit(10).toPandas().head(3)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"ename": "SyntaxError", "evalue": "invalid syntax (<ipython-input-3-656c9296be26>, line 1)", "output_type": "error", "traceback": ["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-656c9296be26>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val data = Seq((1,2,3), (4,5,6), (6,7,8), (9,19,10))\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}], "source": "val data = Seq((1,2,3), (4,5,6), (6,7,8), (9,19,10))\nval ds = spark.createDataset(data)\nds.show()"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://jupyter-test-m.us-central1-f.c.avid-reference-267100.internal:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f36443c36a0>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "import findspark\n\nfindspark.init()\n\nimport pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\nspark"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}